\chapter{Theoretical Background} 	% Produces section heading.  Lower-level
\label{theoretical-background}

%Basics such as theory, definitions, relevant theories, related work and state of the art should be included here.

{\color{red}TODO}

% maximal 1.1.1 
% wenns 1.1 gibt, muss es auch 1.2 geben

\section{General Definitions}
\label{theoretical-background:general-definitions}

The following section includes general definitions of terms on the topic.

\subsection*{Cloud Computing}

\begin{quotation}
\noindent
\enquote*{Cloud computing is a model for enabling ubiquitous, convenient, on-demand network access to a shared
	pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that
	can be rapidly provisioned and released with minimal management effort or service provider interaction.
	This cloud model is composed of five essential characteristics, three service models, and four deployment
	models.}
\autocite{cloudComputingNistDefinition2011}
\end{quotation}

The characteristics are
\autocite{cloudComputingNistDefinition2011}:

\begin{itemize}
	\item On-demand self-service
	\item Broad network access
	\item Resource pooling
	\item Rapid elasticity
	\item Measured service
\end{itemize}

The service models are
\autocite{cloudComputingNistDefinition2011}:

\begin{itemize}
	\item Software as a Service (SaaS)
	\item Platform as a Service (PaaS)
	\item Infrastructure as a Service (IaaS)
\end{itemize}

The deployment models are
\autocite{cloudComputingNistDefinition2011}:

\begin{itemize}
	\item Private cloud
	\item Community cloud
	\item Public cloud
	\item Hybrid cloud
\end{itemize}

\subsection*{Cloud Native Application}

\begin{quotation}
\noindent
\enquote*{A cloud-native application (CNA) is a distributed, elastic and horizontal scalable system composed of (micro)services which isolates state in a
minimum of stateful components. The application and each self-contained
deployment unit of that application is designed according to cloud-focused
design patterns and operated on a self-service elastic platform.}
\autocite{cloudNativeApplicationDefinition2017}
\end{quotation}

\subsection*{Continuous}
\begin{quotation}
\noindent
\enquote*{"Continuous" is intended to match the industry standard term: reconciliation continues to happen, not that it must be instantaneous.}
\autocite{gitopsGlossary}
\end{quotation}

\subsection*{Declarative Description}
\begin{quotation}
\noindent
\enquote*{A configuration that describes the desired operating state of a system without specifying procedures for how that state will be achieved. This separates configuration (the desired state) from the implementation (commands, API calls, scripts etc.) used to achieve that state.}
\autocite{gitopsGlossary}
\end{quotation}

\subsection*{Desired State}
\begin{quotation}
\noindent
\enquote*{The aggregate of all configuration data that is sufficient to recreate the system so that instances of the system are behaviourally indistinguishable. This configuration data generally does not include persistent application data, eg. database contents, though often does include credentials for accessing that data, or configuration for data recovery tools running on that system.}
\autocite{gitopsGlossary}
\end{quotation}

\subsection*{DevOps}

\citeauthor{devopsDefinition2016} (\citeyear{devopsDefinition2016})
define DevOps as
a development methodology aimed at bridging the gap between
development and operations, emphasizing communication and collaboration,
continuous integration, quality assurance and delivery with automated deployment
utilizing a set of development practices
\autocite{devopsDefinition2016}.

\subsection*{Drift}
\begin{quotation}
\noindent
\enquote*{When a system's actual state has moved or is in the process of moving away from the desired state, this is often referred to as drift.}
\autocite{gitopsGlossary}
\end{quotation}

\subsection*{Environment}

An Environment
- or GitOps Environment -
in the context of this thesis
is defined as a target deployment environment for a given application;
e.g. Development, Testing, or Production.
Most of the time this is a Kubernetes cluster or namespace.

In the context of the proposed Kubernetes Custom Resource Definition
Environment, however it represents a folder/directory in a Git repository,
which points to a deployment environment or cluster/namespace as defined
in the previous paragraph.

\subsection*{Feedback}
\begin{quotation}
	\noindent
	\enquote*{Open GitOps follows control-theory and operates in a closed-loop. In control theory, feedback represents how previous attempts to apply a desired state have affected the actual state. For example if the desired state requires more resources than exist in a system, the software agent may make attempts to add resources, to automatically rollback to a previous version, or to send alerts to human operators.}
	\autocite{gitopsGlossary}
\end{quotation}

\subsection*{GitOps}

This thesis aims at adhering to the definition of the term GitOps,
%for the term GitOps, its principles and the glossary around it.
as specified by the CNCF project OpenGitOps.
The overall goal of OpenGitOps is to establish a clear vendor-neutral,
principle-driven meaning of GitOps,
which shall provide a foundation for interoperability between tools, conformance and certification through enduring programs, documents and code
\autocite{opengitopsDocuments}.

\noindent
\enquote*{The primary four principles
	for the desired state of a
	system managed by GitOps are the following} \autocite{gitopsPrinciplesv100}:

\begin{itemize}
	\item \textbf{Declarative} \\
	\enquote*{A system managed by GitOps must have its desired state expressed declaratively.}
	\item \textbf{Versioned and Immutable} \\
	\enquote*{Desired state is stored in a way that enforces immutability, versioning and retains a complete version history.}
	\item \textbf{Pulled Automatically} \\
	\enquote*{Software agents automatically pull the desired state declarations from the source.}
	\item \textbf{Continuously Reconciled} \\
	\enquote*{Software agents continuously observe actual system state and attempt to apply the desired state.}
	\autocite{gitopsPrinciplesv100}
\end{itemize}

% ensure list does not span over multiple pages in PDF export

These principles are defined in OpenGitOps version 1.0.0,
along with glossary
\autocite{gitopsGlossary}
for associated terms and concepts.
%The ones needed for understanding of this thesis,
%are the following:

\subsection*{Internal Developer Platform}

\begin{quotation}
	\noindent
	\enquote*{An Internal Developer Platform (IDP) is built by a platform team to build golden paths and enable developer self-service. An IDP consists of many different techs and tools, glued together in a way that lowers cognitive load on developers without abstracting away context and underlying technologies. Following best practices, platform teams treat their platform as a product and build it based on user research, maintain and continuously improve it.}
	\autocite{internaldeveloperplatformWhatIsIDP}
\end{quotation}

\subsection*{Phase scheme}
The ideal-typical structure of a project in sections of logically related tasks including the methodology,
methods and techniques of task solution. Synonym: phase model
\autocite{riedlManagementInformatik2019}.

\subsection*{Platform Engineering}

Platform Engineering represents the engineering processes
for providing an internal developer platform (as defined in this section).

\subsection*{Promotion}

Promotion in the context of this thesis is defined as
the process of promoting a new application or infrastructure version (release)
to another deployment environment.
In the context of GitOps and Git repositories,
this often means changing declarative definitions of the desired state in Git repositories.

\subsection*{Prototype}
An executable model of the planned product, produced with little effort and easy to modify,
which can be tested and evaluated by the future user
\autocite{riedlManagementInformatik2019}.

\subsection*{Prototyping}
The entirety of activities, methods and tools required to produce prototypes
\autocite{riedlManagementInformatik2019}.

\subsection*{Prototyping cycle}
A sequence of steps consisting of using, evaluating and modifying a prototype
\autocite{riedlManagementInformatik2019}.





\subsection*{Reconciliation}
\begin{quotation}
\noindent
\enquote*{The process of ensuring the actual state of a system matches its desired state. Contrary to traditional CI/CD where automation is generally driven by pre-set triggers, in GitOps reconciliation is triggered whenever there is a divergence. Divergence could be due to the actual state unintentionally drifting from the desired state declarations, or a new desired state declaration version having been changed intentionally. Actions are taken based on policies around feedback from the system and previous reconciliation attempts, in order to reduce deviation over time.}
\autocite{gitopsGlossary}
\end{quotation}

\subsection*{Release}

A release in the context of this thesis
represents the process of
publishing a new version of an application or software component
to the users.
When following GitOps practices,
this usually means
pushing a new Git tag to a Git repository,
which triggers a CI/CD workflow,
which as one of its steps publishes the software artifact of
the new version of the application in an artifact registry.

\subsection*{Software System}
\begin{quotation}
\noindent
\enquote*{A software system managed by GitOps includes} \autocite{gitopsGlossary}:
\begin{itemize}
	\item \enquote*{One or more runtime environments consisting of resources under management}
	\item \enquote*{The management agents within each runtime}
	\item \enquote*{Policies for controlling access and management of repositories, deployments, runtimes}
	\autocite{gitopsGlossary}
\end{itemize}
\end{quotation}

\subsection*{State Store}
\begin{quotation}
\noindent
\enquote*{A system for storing immutable versions of desired state declarations. This state store should provide access control and auditing on the changes to the Desired State. Git, from which GitOps derives its name, is the canonical example used as this state store but any other system that meets these criteria may be used. In all cases, these state stores must be properly configured and precautions must be taken to comply with requirements set out in the GitOps Principles.}
\autocite{gitopsGlossary}
\end{quotation}













\section{GitOps Related Tooling and Components}
% Configuration Management Tools, GitOps Engines, and Git Providers

In the following section,
the role of GitOps related tooling and components, namely
configuration management tools,
GitOps engines,
and Git providers
are explained.

\subsection*{Configuration and Templating Tools}

An important component of a GitOps toolchain is the configuration management or templating tool
for the configuration files, i.e. Kubernetes manifests.
Since the Kubernetes manifests are declarative and highly configurable in nature,
their configuration and customization can be rather cumbersome.
Many definitions are duplicated, so it is desirable to use variables, templates and the like,
for making the configuration easier to use and maintain.
Popular tools are Kustomize, Helm, cdk8s and Carvel ytt were created to help with configuration and templating.

\enquote*{Kustomize introduces a template-free way to customize application configuration that simplifies the use of off-the-shelf applications. Now, built into kubectl as apply -k.}
\autocite{kustomizeIoWebsite}

Kustomize provides a way to customize base Kubernetes manifests with minimal additional overhead.
It is built into kubectl and works purely declarative with raw manifests as input and output.

Helm is the de-facto standard package manager for Kubernetes.
It provides a way to package the configuration and easily deploy those packages
which can be highly configurable.
Most third-party applications offer a helm chart for installation.
It is based on using templates with variables and minimal logic,
which can be rendered into plain Kubernetes manifests, usually at deployment time,
as variables may be input for last-mile configuration.

Cdk8s is a development kit for Kubernetes.
It
\enquote*{is an open-source software development framework for defining Kubernetes applications and reusable abstractions using familiar programming languages and rich object-oriented APIs. cdk8s apps synthesize into standard Kubernetes manifests.}
\autocite{cdk8sWebsite}

Carvel ytt is used to template and patch any YAML definitions.
It strives to function deterministically,
meaning the
\enquote*{ytt execution environment is hermetic and side-effect free, with no access to filesystem, network, time, randomness, or the operating system interfaces. This guarantees that templates produce identical output with the same inputs. Your configuration changes only when you change it.}
\autocite{carvelYttWebsite}









\subsection*{GitOps Engines}

The GitOps engine or controller is responsible for the
reconciliation of the desired state with the actual state
in the target deployment environment.
It adheres to the GitOps principles and is the primary tool
to achieve the GitOps pattern.

The different alternative GitOps engines offer similar functionality,
and they have their own advantages and disadvantages.
When extending the GitOps toolchain, it should not make a difference which specific
tool from which provider is used.
GitOps engines should work together with any other tool in the GitOps ecosystem.

The most widely adopted GitOps engines and accompanying projects are those from
the Argo
\autocite{argoProjWebsite}
and Flux
\autocite{fluxWebsite}
projects.








\subsection*{Git Providers}

Git providers or otherwise called Git servers are a relevant component of
a GitOps setup.
For the most important Git functionalities like branches, commits, history
tags, cherry-picking or merges, each Git provider functions the same.
However, one of the most important functions which is often desirable with GitOps, is the Git pull request.
As pull requests are not part of the open-source core of Git,
each Git provider offers a different API for the pull requests.
Integrations with the pull request API from Git providers therefore need to be implemented
for each Git provider separately. Thus many software tools support only certain Git providers.

Some of the most prominent Git providers are
Github,
Gitlab,
Bitbucket,
Azure DevOps, and
Gitea.










\section{DevOps \& Internal Developer Platforms}

DevOps, as defined in section \ref{theoretical-background:general-definitions},
is seeing increasingly more adoption amongst organizations.
One of the most important principles of DevOps is
to allow the developer who brings new code changes
which end up in new product releases,
to have as much insight into the software development lifecycle as possible.
So it is not just bringing developers and operations closer together,
but to shift many processes into the developers hands.
This is to give developers as much insight as possible,
in order to decrease efficiency and productivity to eventually
decrease the time-to-market for new product releases, features and bug fixes.
This is essentially needed for organizations to continue to thrive in todays
rapidly changing world.

Cloud native technologies have allowed for this movement to happen.
An important aspect of cloud technologies is the self-service.
When previously it was necessary to have many different teams and departments
within a software development organization,
each being responsible for individual parts of the 
software development lifecycle,
it is now easier than ever before possible to reduce the amount of
teams down to a minimum, in order to reduce friction between people communications and processes.
This additionally gives the developers much better insights of what effect their code changes have
on the end product or service, which users consume.

In order to increase the level of self-service for developers,
and increase their productivity,
many organizations implement the concept of internal developer platforms.
While this concept has been around for quite a while,
it has now in recent years explicitely gotten more meaning and attention.

Internal Developer Platforms have received a lot of attention
when Spotify released their project Backstage
\autocite{backstageIOWebsite}
with an open-source license in 2020. The project was accepted into the CNCF in 2022.

%
Backstage was initially created at Spotify, due to the need for it.
As Spotify grew and added increasingly more microservices to their software catalog,
their developers were observed to becoming less and less productive.
Engineering teams at Spotify were spending too much time context switching between
a burden of tasks, which were the result of bad organization of all their
information around applications, microservices, infrastructure, etc.
What was actually valuable, building, testing and releasing code,
was becoming less frequent and more difficult to achieve over time.
Engineers at Spotify decided that they needed to make it easier 
for their developers to do their work
\autocite{backstageSpotifyStory}.

Their idea was to
\enquote*{centralize and simplify end-to-end software development
with an abstraction layer that sits on top of all of our infrastructure and developer tooling.}
\autocite{backstageSpotifyStory}

The resulting product Backstage,
is a developer portal with a centralized software catalog,
with a pluggable architecture,
which allows for extensibility and customization.
Services and software tooling can be managed via the Backstage platform.
To summarize, the platform makes it easier for developers to create and maintain
microservices, keep track of service owners, and the like
\autocite{backstageSpotifyStory}.

The term Platform Engineering basically refers to this concept of
providing internal developer platforms to increase developer productivity
in todays world of distributed microservice applications,
and dozens of languages and frameworks,
and tools to choose from.






\section{Git + Ops Is Not GitOps}

Infrastructure-as-Code which is stored in a Git repository,
and executed by a CI/CD pipeline,
is not actually the concept behind GitOps.

A visual representation of what is not GitOps can be seen in
fig. \ref{fig:iacIsNotGitOps}.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.00\linewidth]{assets/iac-is-not-gitops.png}
	\caption{IaC is not GitOps.
		%		(\citeauthor{ref}, \citeyear{ref}).
	}
	\label{fig:iacIsNotGitOps}	
\end{figure}

Infrastructure-as-Code is only one aspect of GitOps, namely the declarative one.
There are three more principles of GitOps.
The definition of GitOps
%as per the OpenGitOps project
is stated in section \ref{theoretical-background:general-definitions} of this thesis.

To avoid confusion with what GitOps actually is, 
the OpenGitOps project
\autocite{openGitOpsProject}
was created within the CNCF;
which is maintained by the GitOps working group.

The overall goal of OpenGitOps is to establish a clear vendor-neutral,
principle-driven meaning of GitOps,
which shall provide a foundation for interoperability between tools, conformance and certification through enduring programs, documents and code
\autocite{opengitopsDocuments}.

In fig. \ref{fig:gitOpsConcept} a visual representation can be seen of the main GitOps concept.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.00\linewidth]{assets/gitops-concept.png}
	\caption{GitOps concept.
		%		(\citeauthor{ref}, \citeyear{ref}).
	}
	\label{fig:gitOpsConcept}	
\end{figure}

Moreover, unlike DevOps,
\begin{quotation}
\noindent
\enquote*{GitOps is not intended to tackle the fuzzier area of social interactions and culture. [...] GitOps is ultimately meant to be a technology pattern, not a social practice. [...] Fundamentally, all the social aspects should be derived from the technical principles. Git is designed as a team tool and gives you a social context, because you have metadata, committed workflows, and group activities. But GitOps is not supposed to be a social movement. [...] With GitOps, all changes to infrastructure and application deployment are versioned, immutable, auditable, and reversible. [...] GitOps is about automating operations and shifting left, so that developers can perform operations tasks without having to always understand it in detail. [...] With GitOps, developers use tools like git to cause operational changes in a robust manner, allowing platform teams to focus on core platform engineering and operations, and leaving developers to focus on product, customers, rapid development, user satisfaction, and so on.}
\autocite{definingGitOpsGithubReadmeFeatured}
\end{quotation}

To conclude, Infrastructure-as-Code is not GitOps. IaC is just one aspect of GitOps.








\section{How GitOps changed Continuous Deployment}
\label{theoretical-background:gitops-cd}

With the GitOps approach,
Continuous Deployment works differently than with push-based CI/CD deployments.
In the following,
push-based and pull-based deployments in the context of GitOps are explained.

\subsection*{Push-based Deployments}

Without GitOps, Continuous Deployment was primarily push-based.
This means, that a code change introduced as a commit to a Git repository by a developer,
passes through each step in a CI/CD pipeline sequentially.
Basically, a single process executes all tasks one after another,
and has the knowledge of where the process is at at a given moment,
and where it fails, and if it succeeds, it knows the status.
These tasks might be automated tests of all sorts,
automated builds of artifacts and finally some sort of uploading of the artifact to a registry.

When Continuous Deployment was desired with this push-based approach,
then a task would be appended at the end of the pipeline,
which would deploy the new artifact to the target deployment environment.
The CI/CD system has knowledge over the status of the deployment, whether it failed or succeeded.

If multiple environments or stages were desired,
the another task would be appended to the pipeline
in a consecutive manner.
If some task fails during a pipeline run,
the pipeline would be cancelled.
This means that a commit, that fails certain automated tests,
whill never be packaged into an artifact.
Conversely, an artifact, that fails to deploy to a certain environment,
will most likely not be deployed to consecutive environments, because of the stopped pipeline.

What is trickier if an artifact is successfully deployed,
and looks like it works to the system,
but in reality the user-facing service is actually not working successfully or with lower quality standards.

The push-based deployment with CI/CD runs synchronously in one process.
This is illustrated in fig. \ref{tikz:push-based-cd}.

With current GitOps tooling,
such a push-based deployment process can technically be implemented,
however it is not advisable, as it violates the \enquote*{pulled automatically} principle of GitOps.


%\begin{figure}[h]
%	\centering
%	\includegraphics[width=1.00\linewidth]{assets/push-based-cd.png}
%	\caption{Push based Continuous Deployment.
	%		%		(\citeauthor{ref}, \citeyear{ref}).
	%	}
%	\label{fig:pushBasedCD}	
%\end{figure}

\begin{figure}
	\centering
	
	\noindent
	\begin{minipage}{.5\textwidth}
		\centering
		\begin{tikzpicture}[node distance=1.3cm]
			\node (start) [startstop] {Start};
			\node (one) [process, below of=start] {commit};
			\node (two) [process, below of=one] {test};
			\node (three) [process, below of=two] {build \& push artifact};
			\node (four) [process, below of=three] {deploy artifact};
			\node (stop) [startstop, below of=four] {Stop};
			
			\draw [arrow] (start) -- (one);
			\draw [arrow] (one) -- (two);
			\draw [arrow] (two) -- (three);
			\draw [arrow] (three) -- (four);
			\draw [arrow] (four) -- (stop);
		\end{tikzpicture}
		\captionof{figure}{Push-based deployment.}
		\label{tikz:push-based-cd}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\begin{tikzpicture}[node distance=1.3cm]
			\node (start) [startstop] {Start};
			\node (one) [process, below of=start] {commit};
			\node (two) [process, below of=one] {test};
			\node (three) [process, below of=two] {build \& push artifact};
			\node (four) [process, below of=three, yshift=-0.8cm] {detect new desired state};
			\node (five) [process, below of=four] {GitOps engine};
			\node (six) [process, below of=five] {deploy artifact};
			\node (stop) [startstop, below of=six] {Stop};
			
			\draw [arrow] (start) -- (one);
			\draw [arrow] (one) -- (two);
			\draw [arrow] (two) -- (three);
			\draw [arrow] (five) -- (four);
			\draw [arrow] (five) -- (six);
			\draw [arrow] (six) -- (stop);
		\end{tikzpicture}
		\captionof{figure}{Pull-based deployment.}
		\label{tikz:pull-based-cd}
	\end{minipage}
	
\end{figure}

\subsection*{Pull-based Deployments}

With the GitOps approach and appropriate tools which implement its principles and pattern,
the deployment process works in a pull-based manner.
This means, that the GitOps engine, which often lives inside the deployment environment
continuously watches the desired state for changes,
and if changes occur, the new desired state is reconciled with the actual state, in order for them to match again.

Now because the deployment process is not done by a task at the end of the CI/CD pipeline,
the pipeline ends with a successful push of the artifact to a registry.
At this point, typically a version updating tool like e.g. Flux Image Update Automation, which notices, that a new version is available in
the artifact registry, patches that new version into the desired state stored in the GitOps repository.
Next, the GitOps engine notices the change in the desired state and does the reconciliation,
which finally ends up in a deployment.
The processes that are responsible for deployment with the GitOps approach run asynchronously.










\section{Promoting Releases Across Environments}

With the push-based pipelines, described in section
\ref{theoretical-background:gitops-cd},
it is typically more straight-forward of how an application
is deployed to multiple target environments.
When the pipeline job to deploy to environment A was successfully done,
the next job to deploy to environment B would be executed.
Since the push-based workflow was a synchronous process from commit
to deployment to target environment,
the whole process could be packed into a single pipeline.

However, with the GitOps workflow the process from
commit to deployment to target environment is usually split up after
publishing of the artifact to the artifact registry.
At this point, the process continues asynchronously.

There are multiple ways how the promotion across environments can be achieved.
In the simplest form, the desired state could be stored in a Git repository,
and the GitOps engine would synchronize every new commit, which is detected -
meaning each and every new change to the desired state is immediately
applied to the actual system state.
For each environment, there would be a representing folder in the Git repository,
or a separate repository.
On every new commit, the reconciliation immediately happens.
It could also be done differently, in that the state or source controller
is configured to watch a specific git refspec, meaning a certain Git tag, for example.
This way, not every commit is deployed, but rather only specific Git tags,
which may be created manually by a human.
When having the desired state as fixed Git tags,
it can possibly provide for more control over what is deployed.

Additionally to leveraging Git tags as fixed and immutable versions,
the desired state could be packaged and pushed to an artifact registry.
Especially when using configuration or templating tools like Kustomize or Helm,
where the Kubernetes manifests need to be rendered before being applied,
it could make sense to go with this approach.
Packaging the desired state into an immutable artifact can also increase
security, when the artifact is signed with a digital signature, and also validated upon deployment.

Whether the desired state is packaged as an artifact with an immutable version and signed signature,
or just stored in a Git repository and secured purely by the built-in Git commit functionality, providing
unique snapshots, which can be rolled back any time;
it always comes down to changing plain text files stored somewhere.
For promotion to other environments, these changes are typically done again for the desired state of the other environment.
There is no way around changing files in Git repositories, as this is within the nature of the GitOps approach.






\section{Towards Progressive Delivery \& Short-Living Environments}

With progressive delivery tools like
Flagger
\autocite{flaggerWebsite}
and
Argo Rollouts
\autocite{argoRolloutsWebsite},
which offer advanced deployment strategies
like canary and blue/green deployments, or A/B testing,
it is now easier to ensure a bad release does not impact the end users
as drastically.
As an example, the named tools make it possible to release new versions
to 10 percent of a specific region of end users,
then this canary rollout is automatically tested and evaluated against metrics,
if certain objectives for metrics fail to be met,
the new release can automatically be rolled back.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.00\linewidth]{assets/progressive-delivery.png}
	\caption{Progressive Delivery.
		%		(\citeauthor{ref}, \citeyear{ref}).
	}
	\label{fig:progressive-delivery}	
\end{figure}

Since the progressive delivery tools allow for a more
fine-grained segmentation
of a single environment based on numerous parameters like
client device type, user region, user type (e.g., developer, admin),
registered or unregistered users, etc.
the requirements to have a multitude of deployment environments
have decreased for some organizations.
An illustration can be seen in fig. \ref{fig:progressive-delivery}.

The described segmentation of an environment with the progressive delivery
tools, however might not be possible for every use case or organization.
Financial institutions for example, where the requirement for absolutely zero
errors hitting the production environment is top priority and business critical,
to say the least,
may still want to run multiple environments next to progressive delivery tools,
to ensure an even higher quality and level of caution.
This is illustrated in fig. \ref{fig:deploy-multiple-envs}.

Multiple environments typically mean a big increase in costs;
with progressive delivery the need for multiple environments has decreased,
and this way costs can be reduced.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.00\linewidth]{assets/deploy-multiple-envs.png}
	\caption{Deploying to multiple environments.
		%		(\citeauthor{ref}, \citeyear{ref}).
	}
	\label{fig:deploy-multiple-envs}	
\end{figure}

Nowadays with the rapid development and releasing of new versions,
there has become a need for dynamic, short-living environments.
For every commit of an individual developer,
a deployment environment may be provisioned for previewing the changes
in a live environment that resembles the production environment as well as possible.
This short-living environment may be deleted after a short specified amount of time
has passed.






\section{Beyond container orchestration with Kubernetes}
\label{theoretical-background:kubernetes}

The following section is about
the role Kubernetes plays in the current cloud native ecosystem,
and its extensible architecture.

Since the birth of the cloud native computing foundation (CNCF) in 2015 and the release of Kubernetes as the first CNCF project,
there have emerged more than a hundred projects.
Most of the projects are hosting cloud native technologies and tools to support Kubernetes.
The current ecosystem and toolchains are strongly centered around Kubernetes,
although not strictly tied to it and often times with the explicit effort
for the ability to support alternative cloud native platforms and solutions.

\enquote*{Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.}
\autocite{kubernetesIoWebsite}
Although this is the primary use case of Kubernetes,
and the reason why it was created initially,
Kubernetes is increasingly being used as a base cloud native platform,
to build other applications and platforms on top of.
The architecture of Kubernetes provides a solid framework and platform,
which is easily extensible.
Developers may extend its API by specifying custom resources and controllers.

There are several advantages when extending the Kubernetes API,
in comparison to a plain REST API.
Some of those are the following
\autocite{kubebuilderBookWebsite}:

\begin{itemize}
	\item \enquote*{Hosted API endpoints, storage, and validation.}
	\item \enquote*{Rich tooling and clis such as kubectl and kustomize.}
	\item \enquote*{Support for Authn and granular Authz.}
	\item \enquote*{Support for API evolution through API versioning and conversion.}
	\item \enquote*{Facilitation of adaptive / self-healing APIs that continuously respond to changes in the system state without user intervention.}
	\item \enquote*{Kubernetes as a hosting environment}
	\autocite{kubebuilderBookWebsite}
\end{itemize}

When developing a Kubernetes-native application,
many of the common capabilities which are often required for all applications,
are being provided by Kubernetes itself, or otherwise easily consumable and integrated.
These may include resource quotas, observability, monitoring, logging and tracing,
configuration state storage, declarative APIs, control loops, and event and message queueing.

\subsection*{Extending Kubernetes}
% https://kubernetes.io/docs/concepts/extend-kubernetes/
Kubernetes offers several different extension points and extension patterns.
Most extension patterns however share the same basic design and principles.
In general, a custom extension is a program which reads and/or writes
to the Kubernetes API. By doing that it can provide useful automation.
Since Kubernetes is based around a declarative API,
where resources are defined as the desired state,
and controllers are responsible for continuously reconciling this
desired state with the actual state,
it has shown to be a good pattern to design custom extensions in the same way
\autocite{extendKubernetes}.

\subsection*{Custom Resources, Controllers and Operators}
% https://kubernetes.io/docs/concepts/architecture/controller/
The concept of a controller and control loop within Kubernetes refers to the
meaning from robotics and automation, where
\enquote*{a control loop is a non-terminating loop that regulates the state of a system.}
\autocite{controllersKubernetes}
Controllers in Kubernetes are continuously running in a control loop,
in specified intervals and sometimes internal or external triggers, 
and watch the actual state of the cluster,
and make changes to it by interacting with the API,
in order to bring the actual state closer to the desired state,
like specified in the declarative definition.
It is typically a good practice to have one controller
be responsible for one resource, 
in order to help with separation of concerns.
Controllers make changes to resources inside the cluster,
like pods and deployments,
but can also be responsible for resources external to the cluster,
like APIs of the infrastructure provider
\autocite{controllersKubernetes}.
A typical controller implementation can be seen in figure
\ref{fig:typicalControllerKubernetes}.
As an example here, the deployment controller continuously ensures the desired state
of three replicas; if it notices the desired state and actual state differ
from one another, it does necessary actions to make them match again.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.00\linewidth]{assets/typical-controller.png}
	\caption{typical controller in Kubernetes.
		%		(\citeauthor{ref}, \citeyear{ref}).
	}
	\label{fig:typicalControllerKubernetes}	
\end{figure}

When a controller has specific domain knowledge,
or does certain tasks, which would usually be done by a human "operator",
it is called an operator.

% https://kubernetes.io/docs/concepts/extend-kubernetes/operator/
Operators typically are a set of controllers and custom resources
with specific codified domain knowledge.
All operational tasks -
which would otherwise have to be done by a human operator -
are written in code.
This code, the controller logic, can then be automated.
Examples for such operational tasks are
backups and restoring of backups, error remediation, database migrations, etc.
\autocite{operatorWhitepaperV1}.
In overly simplified terms:
An operator is a controller plus domain-specific operational knowledge
(fig. \ref{fig:operatorAndController}).

\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\linewidth]{assets/operator-is-controller-domain-knowledge.png}
	\caption{Operator and Controller.
		%		(\citeauthor{ref}, \citeyear{ref}).
	}
	\label{fig:operatorAndController}	
\end{figure}

The Operator Design Pattern represents a set of principles about
managing complex applications and/or infrastructure resources,
using domain-specific knowledge.
The goal is to limit any manual work that needs to be done,
and try to automate all operational tasks.
This is done by capturing domain-specific knowledge in code,
defining the desired state of resources and exposing them
via a declarative API
\autocite{operatorWhitepaperV1}.

To simplify the process of creating and maintaining Kubernetes-native application
in the form of an operator,
there exist several operator frameworks.
The most prominent framework is the Kubebuilder Framework.
%\url{https://github.com/kubernetes-sigs/kubebuilder}
The Kubebuilder framework makes the process of extending the Kubernetes API
an easy process for developers.
An initial project can easily be boostrapped, allowing the developer to focus
on implementing the custom resource definitions and controller logic.
Any needed Kubernetes primitives, such as service accounts and RBAC permissions
are automatically generated.
Documentation for the OpenAPI resources are also generated from the code,
which is the Go programming language.
There exist rich libraries for interfacing with Kubernetes components,
since Kubernetes itself is also implemented in the Go language
\autocite{kubebuilderBookWebsite}.

With custom resources, the Kubernetes API can dynamically be extended during runtime,
without the need to access its source code or recompile it.

While a resource is
\enquote*{an endpoint in the Kubernetes API that stores a collection of API objects of a certain kind}
\autocite{customResourcesKubernetesIO},
a custom resource is
\enquote*{an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation}
\autocite{customResourcesKubernetesIO}.
Custom resources can be dynamically registered and independently updated.
Users can interface with its objects as they do with the Kubernetes built-in resources
\autocite{customResourcesKubernetesIO}.









\section{Modeling GitOps Environments}

GitOps environments as defined in section
\ref{theoretical-background:general-definitions},
are currently being modeled using different approaches.
The most prominent approach is to have a folder in a Git repository
per environment. This is straight-forward and easily compatible with
the currently most used configuration and templating tools like Kustomize and Helm.
Promotion would be just a file copy operation from one to another file or folder.
For each environment, or only critical ones, there could be a completely separate Git repository.
Having a separate repository opens up the possibility to have a more strict separation of concerns,
regarding permissions and access rights.
Anyone who has access to a Git repository has read access to the entire repository tree.
While the write access could potentially be limited by administrators or maintainers
to certain folders, the read access will always be open for anyone who has access to the repository.

Another approach is to have a Git branch per environment.
Promotion would be a Git merge from one to another branch.
When taking this approach, and also having environment-specific configuration,
it is possible, when not being careful, that a merge conflict happens,
and the promotion would need to be solved manually.
However when this approach is purely used for the purpose of staging,
meaning environments are identical, but new releases are deployed to some environments first
after other ones,
it could provide a seamless way of promotion by solely leveraging the built-in merge mechanism in Git.
Branches can usually be restricted or limited to certain developers only,
which would make it easier to implement the access permissions than the previous folder-per-environment approach.

This research focuses primarily on the modeling of folder-per-environment.








\section{Promotion Strategies with Existing Tools}

There are many ways for achieving a promotion process when using the GitOps approach for the Continuous Delivery of
an application or any other infrastructure declared in code.
When a push-based CI/CD pipeline is already in place, it is often the first thing that comes to mind
to just append the GitOps deployment as a synchronized process to the end of the pipeline.
GitOps engines like ArgoCD offer the capability for the user to configure webhook receivers
which reconcile an application. This makes it possible to setup a synchronous pipeline,
which results in every code change being delivered through a pipeline step by step in a synchronous manner.
After the push-based synchronous deployment step succeeds in the pipeline process,
the pipeline developer can configure another imperative step which triggers reconcilication for another environment.

However, there is a downside when choosing to go with such a push-based imperative approach.
In fact, this approach would abide by the core principles of GitOps,
namely the principle of desired state being continuously being pulled by the GitOps engine/agent.
So the advantage of drift detection and automatic mending, would no longer be given.

When following the GitOps approach, it may be desirable to decouple Continuous Integration/Delivery and Continuous Deployment.
This means that a code change, namely a Git commit, triggers a Continuous Integration pipeline,
which ends in a step which builds and pushes an artifact of the new version to an artifact registry.
The pipeline ends at this point.
Afterwards the GitOps engine, another system than the CI/CD system, is responsible for detecting the new desired state and starting reconciliation.
However, the desired state first needs to be changed after a new artifact is available,
and especially when multiple environments are needed and promotion should be achieved between them,
this is not straight-forward with this asynchronous process.
It may be desirable to trigger tests after a deployment to a certain environment is done,
and only afterwards promote to another environment.
The GitOps approach proposes to have the source of truth in the Git repository as the desired state of a system.

Currently there is insufficient tooling in the GitOps ecosystem for streamlining such a
multi-environment promotion process.

{\color{red}\larger TODO review section!}



















\section{Summary}

TODO































%
%\section{Instruction included in the original FHBgld word processor template}
%\subsection{General definitions}
%Die in dieser Formatvorlage beispielhaft enthaltenen Überschriften sind auf die im
%konkreten Fall tatsächlich passenden Überschriften anzupassen.
%In diesem Teil der Arbeit werden die zum eindeutigen Verständnis unbedingt
%erforderlichen Grundlagen und Definitionen sowie die Erklärung wichtiger Begriffe
%angeführt.
%Die Gliederungspunkte müssen möglichst prägnant bezeichnet werden.
%\subsection{Related work / state of research}
%Auch die neuesten Entwicklungen und Arbeiten auf diesem Gebiet (Stand der
%Wissenschaft oder auch state-of-the-art) sind darzulegen, wobei diese je nach Thema
%auch in der 1. Gliederungsebene behandelt werden können.
%
%\section{Ordinary text}
%% A '%' character causes TeX to ignore all remaining text on the line,
%% and is used for comments like this one.
%
%% sections are begun with similar 
%% \subsection and \subsubsection commands.
%
%The ends  of words and sentences are marked by spaces. It doesn't matter how many 
%spaces    you type; one is as good as 100.  The
%end of   a line counts as a space.
%
%One   or more   blank lines denote the  end 
%of  a paragraph.  
%
%Since any number of consecutive spaces are treated
%like a single one, the formatting of the input
%file makes no difference to
%\LaTeX,                % The \LaTeX command generates the LaTeX logo.
%but it makes a difference to you.  When you use 
%\LaTeX \cite{lamport94},  % \cite inserts a reference, which you define at the end of the document
%making your input file as easy to read
%as possible will be a great help as you write 
%your document and when you change it.  This sample 
%file shows how you can add comments to your own input 
%file.
%
%Because printing is different from typewriting,
%there are a number of things that you have to do
%differently when preparing an input file than if
%you were just typing the document directly.
%Quotation marks like
%``this'' 
%have to be handled specially, as do quotes within
%quotes:
%``\,`this'            % \, separates the double and single quote.
%is what I just 
%wrote, not  `that'\,''.  
%
%Dashes come in three sizes: an 
%intra-word 
%dash, a medium dash for number ranges like 
%1--2, 
%and a punctuation 
%dash---like 
%this.
%
%A sentence-ending space should be larger than the
%space between words within a sentence.  You
%sometimes have to type special commands in
%conjunction with punctuation characters to get
%this right, as in the following sentence.
%Gnats, gnus, etc.\ all  % `\ ' makes an inter-word space.
%begin with G\@.         % \@ marks end-of-sentence punctuation.
%You should check the spaces after periods when
%reading your output to make sure you haven't
%forgotten any special cases.  Generating an
%ellipsis
%\ldots\               % `\ ' is needed after `\ldots' because TeX 
%% ignores spaces after command names like \ldots 
%% made from \ + letters.
%%
%% Note how a `%' character causes TeX to ignore 
%% the end of the input line, so these blank lines 
%% do not start a new paragraph.
%%
%with the right spacing around the periods requires
%a special command.
%
%\LaTeX\ interprets some common characters as
%commands, so you must type special commands to
%generate them.  These characters include the
%following:
%\$ \& \% \# \{ and \}.
%
%In printing, text is usually emphasized with an
%\emph{italic}  
%type style.  
%
%\begin{em}
%	A long segment of text can also be emphasized 
%	in this way.  Text within such a segment can be 
%	given \emph{additional} emphasis.
%\end{em}
%
%It is sometimes necessary to prevent \LaTeX\ from
%breaking a line where it might otherwise do so.
%This may be at a space, as between the ``Mr.''\ and
%``Jones'' in
%``Mr.~Jones'',        % ~ produces an unbreakable interword space.
%or within a word---especially when the word is a
%symbol like
%\mbox{\emph{itemnum}} 
%that makes little sense when hyphenated across
%lines.
%
%Footnotes\footnote{This is an example of a footnote.}
%pose no problem.
%
%\LaTeX\ is good at typesetting mathematical formulas
%like
%\( x-3y + z = 7 \) 
%or
%\( a_{1} > x^{2n} + y^{2n} > x' \)
%or  
%\( AB  = \sum_{i} a_{i} b_{i} \).
%The spaces you type in a formula are 
%ignored.  Remember that a letter like
%$x$                   % $ ... $  and  \( ... \)  are equivalent
%is a formula when it denotes a mathematical
%symbol, and it should be typed as one.
%Furthermore you can add a formula as Images or Tables, see Formula  \hyperref[eq:abc]{\ref{eq:abc}}
%\begin{equation}
%	\label{eq:abc}
%	a+b=c
%\end{equation}
%
%It is sometimes necessary to prevent \LaTeX\ from
%breaking a line where it might otherwise do so.
%This may be at a space, as between the ``Mr.''\ and
%``Jones'' in
%``Mr.~Jones'',        % ~ produces an unbreakable interword space.
%or within a word---especially when the word is a
%symbol like
%\mbox{\emph{itemnum}} 
%that makes little sense when hyphenated across
%lines.
